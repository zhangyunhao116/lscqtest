// Code generated by go run types_gen.go; DO NOT EDIT.
package lscq

import (
	"sync"
	"sync/atomic"
	"unsafe"
)

var uint64CRQpool = sync.Pool{
	New: func() interface{} {
		return newUint64CRQ(scqsize)
	},
}

type Uint64LCRQ struct {
	head *uint64CRQ
	_    [cacheLineSize - unsafe.Sizeof(new(uintptr))]byte
	tail *uint64CRQ
}

func newLCRQ() *Uint64LCRQ {
	q := newUint64CRQ(scqsize)
	return &Uint64LCRQ{head: q, tail: q}
}

func (q *Uint64LCRQ) Dequeue() (data uint64, ok bool) {
	for {
		cq := (*uint64CRQ)(atomic.LoadPointer((*unsafe.Pointer)(unsafe.Pointer(&q.head))))
		data, ok = cq.Dequeue()
		if ok {
			return
		}
		// cq does not have enough entries.
		nex := atomic.LoadPointer((*unsafe.Pointer)(unsafe.Pointer(&cq.next)))
		if nex == nil {
			// We don't have next CRQ.
			return
		}
		// cq.next is not empty, subsequent entry will be insert into cq.next instead of cq.
		// So if cq is empty, we can move it into ncqpool.
		data, ok = cq.Dequeue()
		if ok {
			return
		}
		if atomic.CompareAndSwapPointer((*unsafe.Pointer)(unsafe.Pointer(&q.head)), (unsafe.Pointer(cq)), nex) {
			// We can't ensure no other goroutines will access cq.
			// The cq can still be previous dequeue's cq.
			cq = nil
		}
	}
}

func (q *Uint64LCRQ) Enqueue(data uint64) bool {
	for {
		cq := (*uint64CRQ)(atomic.LoadPointer((*unsafe.Pointer)(unsafe.Pointer(&q.tail))))
		nex := atomic.LoadPointer((*unsafe.Pointer)(unsafe.Pointer(&cq.next)))
		if nex != nil {
			// Help move cq.next into tail.
			atomic.CompareAndSwapPointer((*unsafe.Pointer)(unsafe.Pointer(&q.tail)), (unsafe.Pointer(cq)), nex)
			continue
		}
		if cq.Enqueue(data) {
			return true
		}
		cq.mu.Lock()
		if atomic.LoadPointer((*unsafe.Pointer)(unsafe.Pointer(&cq.next))) != nil {
			cq.mu.Unlock()
			continue
		}
		ncq := uint64CRQpool.Get().(*uint64CRQ) // create a new queue
		ncq.Enqueue(data)
		// Try Add this queue into cq.next.
		if atomic.CompareAndSwapPointer((*unsafe.Pointer)(unsafe.Pointer(&cq.next)), nil, unsafe.Pointer(ncq)) {
			// Success.
			// Try move cq.next into tail (we don't need to recheck since other enqueuer will help).
			atomic.CompareAndSwapPointer((*unsafe.Pointer)(unsafe.Pointer(&q.tail)), unsafe.Pointer(cq), unsafe.Pointer(ncq))
			cq.mu.Unlock()
			return true
		}
		// CAS failed, put this new CRQ into scqpool.
		// No other goroutines will access this queue.
		ncq.Dequeue()
		uint64CRQpool.Put(ncq)
		cq.mu.Unlock()
	}
}

type uint64CRQ struct {
	_    [cacheLineSize]byte
	head uint64
	_    [cacheLineSize - unsafe.Sizeof(new(uint64))]byte
	tail uint64 // 1-bit finalize + 63-bit tail
	_    [cacheLineSize - unsafe.Sizeof(new(uint64))]byte
	next *uint64CRQ
	ring []crqNodeUint64
	mu   sync.Mutex
}

type crqNodeUint64 struct {
	flags uint64 // isSafe 1-bit + isEmpty 1-bit + cycle 62-bit
	data  uint64
}

func newUint64CRQ(n int) *uint64CRQ {
	if n < 0 || n%2 != 0 {
		panic("n must bigger than zero AND n % 2 == 0")
	}
	ring := make([]crqNodeUint64, n)
	for i := range ring {
		ring[i].flags = 1<<63 + 1<<62 + uint64(i) // newCRQFlags(true, true, 0)
	}
	return &uint64CRQ{
		head: uint64(n),
		tail: uint64(n),
		ring: ring,
	}
}

func (q *uint64CRQ) Enqueue(data uint64) bool {
	for {
		// Increment the TAIL, try to occupy an entry.
		tailvalue := atomic.AddUint64(&q.tail, 1)
		tailvalue -= 1 // we need previous value
		T := uint64Get63(tailvalue)
		if uint64Get1(tailvalue) {
			// The queue is closed, return false, so following enqueuer
			// will insert this data into next CRQ.
			return false
		}
		entAddr := &q.ring[cacheRemap16Byte(T)]
		// Enqueue do not need data, if this entry is empty, we can assume the data is also empty.
		entFlags := atomic.LoadUint64((*uint64)(unsafe.Pointer(entAddr)))
		isSafe, isEmpty, idx := loadSCQFlags(entFlags)
		ent := crqNodeUint64{flags: entFlags}
		if isEmpty && idx <= T && (isSafe || atomic.LoadUint64(&q.head) <= T) {
			// We can use this entry for adding new data if
			// 1. Entry's cycle is bigger than tail's cycle.
			// 2. It is empty.
			// 3. It is safe or tail >= head (There is enough space for this data)
			newEnt := crqNodeUint64{flags: newSCQFlags(true, false, T), data: data}
			// Save input data into this entry.
			if compareAndSwapCRQNodeUint64(entAddr, ent, newEnt) {
				return true
			}
		}
		if T-atomic.LoadUint64(&q.head) >= scqsize {
			atomicTestAndSetFirstBit(&q.tail)
			return false
		}
	}
}

func (q *uint64CRQ) Dequeue() (data uint64, ok bool) {
	for {
		// Decrement HEAD, try to release an entry.
		H := atomic.AddUint64(&q.head, 1)
		H -= 1 // we need previous value
		entAddr := &q.ring[cacheRemap16Byte(H)]
		ent := loadCRQNodeUint64(unsafe.Pointer(entAddr))
		isSafe, isEmpty, idx := loadSCQFlags(ent.flags)
		if idx > H {
			goto Line52
		}
		if !isEmpty {
			if idx == H {
				if compareAndSwapCRQNodeUint64(entAddr, ent, crqNodeUint64{flags: newSCQFlags(isSafe, true, H+scqsize)}) {
					return ent.data, true
				}
			} else { // mark unsafe
				newEnt := ent
				newEnt.flags = ent.flags & (1<<63 - 1)
				if compareAndSwapCRQNodeUint64(entAddr, ent, newEnt) {
					goto Line52
				}
			}
		} else { // idx <= H and isEmpty, try empty transition.
			if compareAndSwapCRQNodeUint64(entAddr, ent, crqNodeUint64{flags: newSCQFlags(isSafe, true, H+scqsize)}) {
				goto Line52
			}
		}
	Line52:
		// check if the queue is empty
		tailvalue := atomic.LoadUint64(&q.tail)
		T := uint64Get63(tailvalue)
		if T <= H+1 {
			// The queue is empty.
			q.fixstate(H + 1)
			return
		}
	}
}

func (q *uint64CRQ) fixstate(originalHead uint64) {
	for {
		head := atomic.LoadUint64(&q.head)
		// if originalHead < head {
		// 	// The last dequeuer will be responsible for fixstate.
		// 	return
		// }
		tailvalue := atomic.LoadUint64(&q.tail)
		if tailvalue >= head {
			return
		}
		if atomic.CompareAndSwapUint64(&q.tail, tailvalue, head) {
			return
		}
	}
}
